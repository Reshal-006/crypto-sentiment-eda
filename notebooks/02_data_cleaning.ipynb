{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c472295",
   "metadata": {},
   "source": [
    "# 02 — Cleaning, Alignment & Feature Engineering\n",
    "This notebook loads the raw price CSVs in `data/raw/` plus the Fear & Greed Index, aligns everything to the shared date window, and engineers per-asset features (returns, moving averages, rolling volatility).\n",
    "\n",
    "Output: `data/processed/merged_clean.csv` (the dashboard reads this file)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da3202",
   "metadata": {},
   "source": [
    "### Load raw inputs\n",
    "Read all `*_prices.csv` files written by Notebook 01 and the Fear & Greed CSV. The loader supports both the older export format and the current clean format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654c78a-f46a-49d4-b806-82f995cb81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_DIR = Path(\"../data/raw\")\n",
    "\n",
    "def load_price_csv(path: str) -> pd.DataFrame:\n",
    "    # New format (recommended): Date + yfinance columns\n",
    "    preview = pd.read_csv(path, nrows=5)\n",
    "    if len(preview.columns) > 0 and str(preview.columns[0]).strip().lower() == \"price\":\n",
    "        # Old format: first 3 rows are header/ticker/date artifacts\n",
    "        df = pd.read_csv(\n",
    "            path,\n",
    "            skiprows=3,\n",
    "            header=None,\n",
    "            names=[\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\"],\n",
    "        )\n",
    "    else:\n",
    "        df = pd.read_csv(path)\n",
    "        if \"Date\" not in df.columns:\n",
    "            raise ValueError(f\"Expected a 'Date' column in {path}. Got columns: {list(df.columns)}\")\n",
    "        # Keep only the columns we care about (some exports include 'Adj Close')\n",
    "        keep = [c for c in [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"] if c in df.columns]\n",
    "        df = df[keep]\n",
    "\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Date\"]).set_index(\"Date\").sort_index()\n",
    "    df.index = df.index.normalize()\n",
    "\n",
    "    # Ensure numerics\n",
    "    for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_fear_greed_csv(path: str) -> pd.DataFrame:\n",
    "    fg = pd.read_csv(path, parse_dates=[\"timestamp\"])\n",
    "    fg = fg.rename(columns={\"timestamp\": \"Date\"}).set_index(\"Date\").sort_index()\n",
    "    fg.index = fg.index.normalize()\n",
    "    fg[\"FG_Value\"] = pd.to_numeric(fg[\"value\"], errors=\"coerce\")\n",
    "    if \"value_classification\" not in fg.columns:\n",
    "        fg[\"value_classification\"] = fg[\"FG_Value\"].apply(\n",
    "            lambda x: \"Extreme Fear\" if x < 25 else (\"Fear\" if x < 45 else (\"Neutral\" if x < 55 else (\"Greed\" if x < 75 else \"Extreme Greed\")))\n",
    "        )\n",
    "    return fg\n",
    "\n",
    "# Load all *_prices.csv saved by 01_data_collection.ipynb\n",
    "price_files = sorted(RAW_DIR.glob(\"*_prices.csv\"))\n",
    "if len(price_files) == 0:\n",
    "    raise FileNotFoundError(f\"No *_prices.csv files found in {RAW_DIR.resolve()}. Run 01_data_collection.ipynb first.\")\n",
    "\n",
    "prices = {}\n",
    "for path in price_files:\n",
    "    ticker = path.stem.replace(\"_prices\", \"\").upper()\n",
    "    prices[ticker] = load_price_csv(str(path))\n",
    "\n",
    "fg = load_fear_greed_csv(str(RAW_DIR / \"fear_greed_index.csv\"))\n",
    "\n",
    "print(\"Loaded assets:\", sorted(prices.keys()))\n",
    "for t, df_ in prices.items():\n",
    "    print(f\"  {t}: {df_.shape}  ({df_.index.min().date()} → {df_.index.max().date()})\")\n",
    "print(f\"  F&G: {fg.shape}  ({fg.index.min().date()} → {fg.index.max().date()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d1e71",
   "metadata": {},
   "source": [
    "### Align to a common date window\n",
    "Find the overlapping range across all selected assets and the Fear & Greed series so comparisons are apples-to-apples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164de46-2aa9-496a-9c5e-1345a4326593",
   "metadata": {},
   "outputs": [],
   "source": [
    "mins = [df_.index.min() for df_ in prices.values()] + [fg.index.min()]\n",
    "maxs = [df_.index.max() for df_ in prices.values()] + [fg.index.max()]\n",
    "\n",
    "common_start = max(mins)\n",
    "common_end = min(maxs)\n",
    "\n",
    "for t in list(prices.keys()):\n",
    "    prices[t] = prices[t].loc[common_start:common_end]\n",
    "fg = fg.loc[common_start:common_end]\n",
    "\n",
    "print(\"Aligned date window:\", common_start, \"→\", common_end)\n",
    "print(\"Aligned shapes:\")\n",
    "for t, df_ in prices.items():\n",
    "    print(f\"  {t}: {df_.shape}\")\n",
    "print(f\"  F&G: {fg.shape}\")\n",
    "\n",
    "# Show a sample\n",
    "first_ticker = sorted(prices.keys())[0]\n",
    "prices[first_ticker].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b8108",
   "metadata": {},
   "source": [
    "### Clean duplicates and missing values\n",
    "Normalize timestamps, remove duplicates, forward-fill small gaps, and ensure numeric columns are ready for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67269a74-d4bf-4f47-80b8-84d3a8757cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates (keep first occurrence)\n",
    "for t in list(prices.keys()):\n",
    "    prices[t] = prices[t][~prices[t].index.duplicated(keep=\"first\")]\n",
    "fg = fg[~fg.index.duplicated(keep=\"first\")]\n",
    "\n",
    "# Forward fill missing values (common for daily financial series)\n",
    "for t in list(prices.keys()):\n",
    "    prices[t] = prices[t].ffill()\n",
    "fg = fg.ffill()\n",
    "\n",
    "# Final numeric enforcement\n",
    "for t in list(prices.keys()):\n",
    "    for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]:\n",
    "        if col in prices[t].columns:\n",
    "            prices[t][col] = pd.to_numeric(prices[t][col], errors=\"coerce\")\n",
    "fg[\"FG_Value\"] = pd.to_numeric(fg[\"FG_Value\"], errors=\"coerce\")\n",
    "\n",
    "# Ensure classification exists\n",
    "if \"value_classification\" not in fg.columns:\n",
    "    fg[\"value_classification\"] = fg[\"FG_Value\"].apply(\n",
    "        lambda x: \"Extreme Fear\" if x < 25 else (\"Fear\" if x < 45 else (\"Neutral\" if x < 55 else (\"Greed\" if x < 75 else \"Extreme Greed\")))\n",
    "    )\n",
    "\n",
    "print(\"Duplicates removed and types fixed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af22305",
   "metadata": {},
   "source": [
    "### Feature engineering (per asset)\n",
    "For each asset, compute daily returns, 7/30-day moving averages, and 30-day rolling volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b62b0-4043-48a3-b515-a93be2a81b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in sorted(prices.keys()):\n",
    "    df_ = prices[t]\n",
    "\n",
    "    # Daily Return\n",
    "    df_[f\"{t}_Return\"] = df_[\"Close\"].pct_change()\n",
    "\n",
    "    # Moving Averages\n",
    "    df_[f\"{t}_MA7\"]  = df_[\"Close\"].rolling(7).mean()\n",
    "    df_[f\"{t}_MA30\"] = df_[\"Close\"].rolling(30).mean()\n",
    "\n",
    "    # Rolling Volatility (30 days)\n",
    "    df_[f\"{t}_Vol30\"] = df_[f\"{t}_Return\"].rolling(30).std()\n",
    "\n",
    "    prices[t] = df_\n",
    "    print(f\"{t} features created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f2f66",
   "metadata": {},
   "source": [
    "### Merge assets + sentiment\n",
    "Inner-join sentiment with each asset’s engineered features on date to produce one tidy analysis table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2aff0-ef17-42e8-a709-788c781f2a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from sentiment and inner-join each asset's engineered features\n",
    "merged = fg[[\"FG_Value\", \"value_classification\"]].copy()\n",
    "\n",
    "for t in sorted(prices.keys()):\n",
    "    df_ = prices[t]\n",
    "    subset = df_[[\"Close\", \"Volume\", f\"{t}_Return\", f\"{t}_MA7\", f\"{t}_MA30\", f\"{t}_Vol30\"]].copy()\n",
    "    subset = subset.rename(columns={\"Close\": f\"Close_{t}\", \"Volume\": f\"Volume_{t}\"})\n",
    "    merged = merged.join(subset, how=\"inner\")\n",
    "\n",
    "print(f\"Merged shape: {merged.shape}\")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada6474",
   "metadata": {},
   "source": [
    "### Save processed dataset\n",
    "Drop the initial rows lost to rolling windows (e.g., the first ~30 days) and write the final dataset to `data/processed/merged_clean.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8674fdf-fddb-4d66-acf2-e39c2c1aaf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN rows created by the rolling windows (first ~30 days)\n",
    "merged_clean = merged.dropna()\n",
    "\n",
    "# Save to processed folder\n",
    "merged_clean.to_csv(\"../data/processed/merged_clean.csv\")\n",
    "\n",
    "print(f\"Final cleaned data saved. Shape: {merged_clean.shape}\")\n",
    "print(\"Columns:\", list(merged_clean.columns))\n",
    "merged_clean.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
